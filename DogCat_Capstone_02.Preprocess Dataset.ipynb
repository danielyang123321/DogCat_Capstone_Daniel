{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low quality sample number: 711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train file number: 24289\n"
     ]
    }
   ],
   "source": [
    "#prepare low-quality sample list and delete them from training dataset\n",
    "#准备低质量的样本清单并进行剔除\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#加载探索过程中的记录文件\n",
    "train_dimensions = pd.DataFrame(pd.read_csv(\"train_dimension.csv\"))\n",
    "train_labels_df = pd.DataFrame(pd.read_csv(\"train_label.csv\"),columns = ['train_file','predict'])\n",
    "train_labels = train_labels_df.values.tolist()\n",
    "\n",
    "#建立低质量样本清单\n",
    "sample_list_low_quality=[]\n",
    "\n",
    "#identify samples which height/width is smaller than 100\n",
    "#识别清晰度较低的图片样本\n",
    "sample_list_low_quality.extend(train_dimensions[train_dimensions.width<100]['train_file'].tolist())\n",
    "sample_list_low_quality.extend(train_dimensions[train_dimensions.height<100]['train_file'].tolist())\n",
    "\n",
    "#identify samples with ambigious lablelling\n",
    "#识别标签模糊的图片样本\n",
    "def label_train_file(train_file_name):\n",
    "    if train_file_name.find('cat')>=0:\n",
    "        return 'cat'\n",
    "    else:\n",
    "        return 'dog'\n",
    "\n",
    "def label_predict(predict):\n",
    "    if predict.find('犬')>=0 or predict.find('狗')>=0 or predict.find('㹴')>=0 or predict.find('梗')>=0 or predict.find('獒')>=0 or predict.find('拉布拉多')>=0 or predict.find('可卡')>=0 or predict.find('柯基')>=0 or predict.find('雪纳瑞')>=0 or predict.find('大丹')>=0 or predict.find('喜乐蒂')>=0 or predict.find('博美')>=0 or predict.find('大白熊')>=0 or predict.find('马尔济斯')>=0 or predict.find('京巴')>=0 or predict.find('大麦町')>=0 or predict.find('比格')>=0 or predict.find('日本狆')>=0 or predict.find('萨摩耶')>=0 or predict.find('中华狼青')>=0 or predict.find('藏狮')>=0 or predict.find('巴西非勒')>=0 or predict.find('奥斯卡贵宾')>=0 or predict.find('郊狼')>=0 or predict.find('雪贵宾')>=0:\n",
    "        return 'dog'\n",
    "    elif predict.find('猫')>=0:\n",
    "        return 'cat'\n",
    "    else:\n",
    "        return 'tbd'\n",
    "\n",
    "for train_label in train_labels:\n",
    "    train_file_name=train_label[0]\n",
    "    predict=train_label[1]\n",
    "    train_file_label=label_train_file(train_file_name)\n",
    "    predict_label=label_predict(predict)\n",
    "    if train_file_label!=predict_label:\n",
    "        sample_list_low_quality.append(train_file_name)\n",
    "\n",
    "#remove duplicate in the low quality sample list\n",
    "#对list进行去重处理\n",
    "sample_list_low_quality = list(set(sample_list_low_quality))\n",
    "print('Low quality sample number: '+str(len(sample_list_low_quality)))\n",
    "\n",
    "#Load training dataset\n",
    "#加载训练图片数据集\n",
    "from sklearn.datasets import load_files     \n",
    "data = load_files('train')\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "train_files = np.array(data['filenames'])\n",
    "train_targets = np_utils.to_categorical(np.array(data['target']), 2)   \n",
    "\n",
    "#exclude low quality samples from the dataset\n",
    "#从训练集中剔除低质量样本\n",
    "train_files_final=[]\n",
    "train_targets_final=[]\n",
    "for train_file,train_target in zip(train_files,train_targets):\n",
    "    if train_file not in sample_list_low_quality:\n",
    "        train_files_final.append(train_file)\n",
    "        train_targets_final.append(train_target)\n",
    "print('final train file number: '+str(len(train_targets_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#function to extract features given image path\n",
    "from keras.preprocessing import image   \n",
    "import numpy as np\n",
    "from keras.applications.nasnet import NASNetLarge, preprocess_input\n",
    "\n",
    "pretrained_model=NASNetLarge(include_top=False, weights='imagenet',pooling='max',input_shape = (331, 331, 3))\n",
    "\n",
    "#convert image to tensors\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(331, 331))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "#extract features\n",
    "def extract_features(train_file):\n",
    "    tensor=path_to_tensor(train_file)\n",
    "    features=pretrained_model.predict(preprocess_input(tensor))\n",
    "    return features.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ea1497f34246d6aca6d674aaabf2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#extract features for each images in the final training set\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "p = IntProgress()\n",
    "progressFull=len(train_files_final)\n",
    "display(p)\n",
    "\n",
    "train_features_final=[]\n",
    "for train_file in train_files_final:\n",
    "    features=extract_features(train_file)\n",
    "    train_features_final.append(features)\n",
    "    p.value = float(len(train_features_final))/float(progressFull)*100\n",
    "    p.description = str(len(train_features_final))+'/'+str(progressFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store extracted features into csv files\n",
    "import pandas as pd\n",
    "train_features_final_df=pd.DataFrame(train_features_final)\n",
    "train_features_final_df.to_csv('train_features_final.csv')\n",
    "train_targets_final_df=pd.DataFrame(train_targets_final)\n",
    "train_targets_final_df.to_csv('train_targets_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25a9ce404a54f4ab42a73e9c6be7dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#extract features for testing images\n",
    "from sklearn.datasets import load_files     \n",
    "data = load_files('test')\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "test_files = np.array(data['filenames'])\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "p = IntProgress()\n",
    "progressFull=len(test_files)\n",
    "display(p)\n",
    "\n",
    "test_features=[]\n",
    "for test_file in test_files:\n",
    "    features=extract_features(test_file)\n",
    "    test_features.append(features)\n",
    "    p.value = float(len(test_features))/float(progressFull)*100\n",
    "    p.description = str(len(test_features))+'/'+str(progressFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store extracted features into csv files\n",
    "import pandas as pd\n",
    "test_features_df=pd.DataFrame(test_features)\n",
    "test_features_df.to_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
